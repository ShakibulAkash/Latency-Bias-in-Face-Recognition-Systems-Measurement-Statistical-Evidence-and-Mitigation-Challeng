{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6527655,"sourceType":"datasetVersion","datasetId":3773762}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install insightface onnxruntime-gpu opencv-python-headless tqdm pandas numpy matplotlib\n!pip -q install glasses-detector\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T23:31:42.186194Z","iopub.execute_input":"2026-01-11T23:31:42.186949Z","iopub.status.idle":"2026-01-11T23:31:49.593977Z","shell.execute_reply.started":"2026-01-11T23:31:42.186921Z","shell.execute_reply":"2026-01-11T23:31:49.593275Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.8/73.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os, time\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm.auto import tqdm\n\nDATA_ROOT = \"/kaggle/input/meglass/MeGlass_ori\"\nassert os.path.exists(DATA_ROOT), \"Path not found. Check Kaggle dataset mount.\"\n\nall_imgs = [os.path.join(DATA_ROOT, f) for f in os.listdir(DATA_ROOT) if f.lower().endswith(\".jpg\")]\nprint(\"Total JPG images:\", len(all_imgs))\nprint(\"Example:\", all_imgs[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T23:37:30.300035Z","iopub.execute_input":"2026-01-11T23:37:30.300719Z","iopub.status.idle":"2026-01-11T23:37:30.372565Z","shell.execute_reply.started":"2026-01-11T23:37:30.300685Z","shell.execute_reply":"2026-01-11T23:37:30.371813Z"}},"outputs":[{"name":"stdout","text":"Total JPG images: 47917\nExample: /kaggle/input/meglass/MeGlass_ori/52422450@N00_identity_1@3236065352_6.jpg\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#Face recognition pipeline (InsightFace)\nfrom insightface.app import FaceAnalysis\n\n# Baseline real-time setting\nDET_SIZE = (640, 640)\n\napp = FaceAnalysis(name=\"buffalo_l\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\napp.prepare(ctx_id=0, det_size=DET_SIZE)\n\nprint(\"InsightFace ready with det_size:\", DET_SIZE)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T23:38:35.601494Z","iopub.execute_input":"2026-01-11T23:38:35.602243Z","iopub.status.idle":"2026-01-11T23:38:53.081305Z","shell.execute_reply.started":"2026-01-11T23:38:35.602215Z","shell.execute_reply":"2026-01-11T23:38:53.080490Z"}},"outputs":[{"name":"stdout","text":"download_path: /root/.insightface/models/buffalo_l\nDownloading /root/.insightface/models/buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 281857/281857 [00:07<00:00, 38632.61KB/s]\n","output_type":"stream"},{"name":"stdout","text":"Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\nset det-size: (640, 640)\nInsightFace ready with det_size: (640, 640)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#Eyeglasses attribute inference (label-free)\n# glasses-detector provides pretrained models for glasses classification/detection/segmentation\n# We'll use its classifier interface (API may vary slightly across versions).\n#Setup glasses detector\nfrom glasses_detector import GlassesClassifier\n\nclf = GlassesClassifier()  # loads pretrained weights\nprint(\"Glasses classifier loaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T23:39:50.076096Z","iopub.execute_input":"2026-01-11T23:39:50.076601Z","iopub.status.idle":"2026-01-11T23:39:54.751953Z","shell.execute_reply.started":"2026-01-11T23:39:50.076571Z","shell.execute_reply":"2026-01-11T23:39:54.751067Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://github.com/mantasu/glasses-detector/releases/download/v1.0.0/classification_anyglasses_shufflenet_v2_x1_0.pth\" to /root/.cache/torch/hub/checkpoints/classification_anyglasses_shufflenet_v2_x1_0.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.96M/4.96M [00:00<00:00, 9.10MB/s]","output_type":"stream"},{"name":"stdout","text":"Glasses classifier loaded.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#Face crop + eyeglasses inference\ndef crop_primary_face(img_bgr, faces):\n    # choose the largest detected face\n    if len(faces) == 0:\n        return None\n    best = max(faces, key=lambda f: (f.bbox[2]-f.bbox[0])*(f.bbox[3]-f.bbox[1]))\n    x1, y1, x2, y2 = best.bbox.astype(int)\n    h, w = img_bgr.shape[:2]\n    x1, y1 = max(0,x1), max(0,y1)\n    x2, y2 = min(w-1,x2), min(h-1,y2)\n    if x2 <= x1 or y2 <= y1:\n        return None\n    face = img_bgr[y1:y2, x1:x2]\n    return face\n\ndef infer_glasses(face_bgr):\n    \"\"\"\n    Returns: (label, confidence)\n    label: 1 = glasses, 0 = no-glasses\n    \"\"\"\n    # classifier expects RGB in many libs; we convert safely\n    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n\n    # glasses-detector classifier typically outputs probabilities per class.\n    # We'll handle both dict and tuple outputs robustly.\n    out = clf.predict(face_rgb)\n\n    # Try common output formats:\n    # - dict: {\"glasses\": p1, \"no_glasses\": p0} or similar\n    # - tuple/list: (label, prob) or (probs array)\n    if isinstance(out, dict):\n        # pick the two-class case\n        keys = [k.lower() for k in out.keys()]\n        if \"glasses\" in keys:\n            p_g = out[list(out.keys())[keys.index(\"glasses\")]]\n            p_ng = out[list(out.keys())[keys.index(\"no_glasses\")]] if \"no_glasses\" in keys else 1 - p_g\n        else:\n            # fallback: take max class\n            items = list(out.items())\n            kmax, pmax = max(items, key=lambda kv: kv[1])\n            label = 1 if \"glass\" in kmax.lower() else 0\n            return label, float(pmax)\n        label = 1 if p_g >= p_ng else 0\n        conf = float(max(p_g, p_ng))\n        return label, conf\n\n    if isinstance(out, (list, tuple)) and len(out) == 2 and isinstance(out[1], (float, int)):\n        return int(out[0]), float(out[1])\n\n    # If it's a numpy array of probs:\n    if hasattr(out, \"shape\"):\n        probs = np.asarray(out).ravel()\n        label = int(np.argmax(probs))\n        conf = float(np.max(probs))\n        # assume index 1 = glasses in 2-class model; if reversed, you'll detect it in sanity check\n        return label, conf\n\n    raise RuntimeError(f\"Unknown classifier output format: {type(out)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-11T23:40:36.104472Z","iopub.execute_input":"2026-01-11T23:40:36.104759Z","iopub.status.idle":"2026-01-11T23:40:36.114203Z","shell.execute_reply.started":"2026-01-11T23:40:36.104733Z","shell.execute_reply":"2026-01-11T23:40:36.113437Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#Label-free cohort building\nimport os, shutil\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nSEED = 42\nrng = np.random.default_rng(SEED)\n\nDATA_ROOT = \"/kaggle/input/meglass/MeGlass_ori\"\nall_imgs = [os.path.join(DATA_ROOT, f) for f in os.listdir(DATA_ROOT) if f.lower().endswith(\".jpg\")]\n\n# scan subset first\nMAX_SCAN = 20000\nscan_list = all_imgs.copy()\nrng.shuffle(scan_list)\nscan_list = scan_list[:min(MAX_SCAN, len(scan_list))]\n\n# temp dir for crops\nCROP_DIR = \"/kaggle/working/face_crops\"\nif os.path.exists(CROP_DIR):\n    shutil.rmtree(CROP_DIR)\nos.makedirs(CROP_DIR, exist_ok=True)\n\nMIN_FACE_SIZE = 80\n\ndef crop_primary_face(img_bgr, faces):\n    if len(faces) == 0:\n        return None\n    best = max(faces, key=lambda f: (f.bbox[2]-f.bbox[0])*(f.bbox[3]-f.bbox[1]))\n    x1, y1, x2, y2 = best.bbox.astype(int)\n    h, w = img_bgr.shape[:2]\n    x1, y1 = max(0,x1), max(0,y1)\n    x2, y2 = min(w-1,x2), min(h-1,y2)\n    if x2 <= x1 or y2 <= y1:\n        return None\n    face = img_bgr[y1:y2, x1:x2]\n    return face\n\ncrop_rows = []\nsaved = 0\n\nfor p in tqdm(scan_list):\n    img = cv2.imread(p)\n    if img is None:\n        continue\n\n    faces = app.get(img)  # InsightFace detector\n    face_crop = crop_primary_face(img, faces)\n    if face_crop is None:\n        continue\n\n    if min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n        continue\n\n    # save crop with stable filename\n    out_name = f\"crop_{saved:06d}.jpg\"\n    out_path = os.path.join(CROP_DIR, out_name)\n    cv2.imwrite(out_path, face_crop)\n\n    crop_rows.append({\"orig_path\": p, \"crop_path\": out_path})\n    saved += 1\n\ndf_crops = pd.DataFrame(crop_rows)\nprint(\"Saved face crops:\", len(df_crops))\ndf_crops.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:07:15.240132Z","iopub.execute_input":"2026-01-12T00:07:15.240955Z","iopub.status.idle":"2026-01-12T00:22:51.540772Z","shell.execute_reply.started":"2026-01-12T00:07:15.240923Z","shell.execute_reply":"2026-01-12T00:22:51.540190Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c791a5d1835740ff87ba76e842860b32"}},"metadata":{}},{"name":"stdout","text":"Saved face crops: 18149\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                           orig_path  \\\n0  /kaggle/input/meglass/MeGlass_ori/51368783@N00...   \n1  /kaggle/input/meglass/MeGlass_ori/96841989@N00...   \n2  /kaggle/input/meglass/MeGlass_ori/10081417@N00...   \n3  /kaggle/input/meglass/MeGlass_ori/40286210@N00...   \n4  /kaggle/input/meglass/MeGlass_ori/16446760@N00...   \n\n                                    crop_path  \n0  /kaggle/working/face_crops/crop_000000.jpg  \n1  /kaggle/working/face_crops/crop_000001.jpg  \n2  /kaggle/working/face_crops/crop_000002.jpg  \n3  /kaggle/working/face_crops/crop_000003.jpg  \n4  /kaggle/working/face_crops/crop_000004.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>orig_path</th>\n      <th>crop_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/51368783@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000000.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/96841989@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000001.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/10081417@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000002.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/40286210@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000003.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/16446760@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000004.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"!glasses-detector --help | head -n 120\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:33:47.872883Z","iopub.execute_input":"2026-01-12T00:33:47.873450Z","iopub.status.idle":"2026-01-12T00:33:52.379951Z","shell.execute_reply.started":"2026-01-12T00:33:47.873421Z","shell.execute_reply":"2026-01-12T00:33:52.379199Z"}},"outputs":[{"name":"stdout","text":"usage: Glasses Detector [-h] -i path/to/dir/or/file [-o path/to/dir/or/file]\n                        [-e <ext>] [-f <format>] [-t <task-name>]\n                        [-s <model-size>] [-b <batch-size>] [-p <pbar-desc>]\n                        [-w path/to/weights.pth] [-d <device>]\n\nClassification, detection, and segmentation of glasses in images.\n\noptions:\n  -h, --help            show this help message and exit\n  -i path/to/dir/or/file, --input path/to/dir/or/file\n                        Path to the input image or the directory with images.\n  -o path/to/dir/or/file, --output path/to/dir/or/file\n                        Path to the output file or the directory. If not\n                        provided, then, if input is a file, the prediction\n                        will be printed (or shown if it is an image),\n                        otherwise, if input is a directory, the predictions\n                        will be written to a directory with the same name with\n                        an added suffix '_preds'. If provided as a file, then\n                        the prediction(-s) will be saved to this file\n                        (supported extensions include: .txt, .csv, .json,\n                        .npy, .pkl, .jpg, .png). If provided as a directory,\n                        then the predictions will be saved to this directory\n                        use `--extension` flag to specify the file extensions\n                        in that directory. Defaults to None.\n  -e <ext>, --extension <ext>\n                        Only used if `--output` is a directory. The extension\n                        to use to save the predictions as files. Common\n                        extensions include: .txt, .csv, .json, .npy, .pkl,\n                        .jpg, .png. If not specified, it will be set\n                        automatically to .jpg for image predictions and to\n                        .txt for all other formats. Defaults to None.\n  -f <format>, --format <format>\n                        The format to use to map the raw prediction to. For\n                        classification, common formats are bool, proba, str,\n                        for detection, common formats are bool, int, img, for\n                        segmentation, common formats are proba, img, mask. If\n                        not specified, it will be set automatically to str,\n                        img, mask for classification, detection, segmentation\n                        respectively. Check API documentation for more\n                        details. Defaults to None.\n  -t <task-name>, --task <task-name>\n                        The kind of task the model should perform. One of\n                        classification, classification:anyglasses,\n                        classification:sunglasses, classification:eyeglasses,\n                        classification:shadows, detection, detection:eyes,\n                        detection:solo, detection:worn, segmentation,\n                        segmentation:frames, segmentation:full,\n                        segmentation:legs, segmentation:lenses,\n                        segmentation:shadows, segmentation:smart. If specified\n                        only as classification, detection, or segmentation,\n                        the subcategories anyglasses, worn, and smart will be\n                        chosen, respectively. Defaults to\n                        classification:anyglasses.\n  -s <model-size>, --size <model-size>\n                        The model size which determines architecture type. One\n                        of 'small', 'medium', 'large' (or 's', 'm', 'l').\n                        Defaults to 'medium'.\n  -b <batch-size>, --batch-size <batch-size>\n                        Only used if `--input` is a directory. The batch size\n                        to use when processing the images. This groups the\n                        files in the input directory to batches of size\n                        `batch_size` before processing them. In some cases,\n                        larger batch sizes can speed up the processing at the\n                        cost of more memory usage. Defaults to 1.\n  -p <pbar-desc>, --pbar <pbar-desc>\n                        Only used if `--input` is a directory. It is the\n                        description that is used for the progress bar. If\n                        specified as '' (empty string), no progress bar is\n                        shown. Defaults to 'Processing'.\n  -w path/to/weights.pth, --weights path/to/weights.pth\n                        Path to custom weights to load into the model. If not\n                        specified, weights will be loaded from the default\n                        location (and automatically downloaded there if\n                        needed). Defaults to None.\n  -d <device>, --device <device>\n                        The device on which to perform inference. If not\n                        specified, it will be automatically checked if CUDA or\n                        MPS is supported. Defaults to None.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"!glasses-detector --help | grep -i \"kind\" -n | head -n 50\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:34:09.450004Z","iopub.execute_input":"2026-01-12T00:34:09.450579Z","iopub.status.idle":"2026-01-12T00:34:13.295885Z","shell.execute_reply.started":"2026-01-12T00:34:09.450543Z","shell.execute_reply":"2026-01-12T00:34:13.295149Z"}},"outputs":[{"name":"stdout","text":"42:                        The kind of task the model should perform. One of\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"OUT_DIR = \"/kaggle/working/gd_out\"\n!rm -rf \"$OUT_DIR\"\n!mkdir -p \"$OUT_DIR\"\n\n!glasses-detector -i \"$CROP_DIR\" -o \"$OUT_DIR\" -t anyglasses-classifier -s small\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:36:16.638373Z","iopub.execute_input":"2026-01-12T00:36:16.639361Z","iopub.status.idle":"2026-01-12T00:36:20.761575Z","shell.execute_reply.started":"2026-01-12T00:36:16.639318Z","shell.execute_reply":"2026-01-12T00:36:20.760639Z"}},"outputs":[{"name":"stdout","text":"usage: Glasses Detector [-h] -i path/to/dir/or/file [-o path/to/dir/or/file]\n                        [-e <ext>] [-f <format>] [-t <task-name>]\n                        [-s <model-size>] [-b <batch-size>] [-p <pbar-desc>]\n                        [-w path/to/weights.pth] [-d <device>]\nGlasses Detector: error: argument -t/--task: invalid choice: 'anyglasses-classifier' (choose from classification, classification:anyglasses, classification:sunglasses, classification:eyeglasses, classification:shadows, detection, detection:eyes, detection:solo, detection:worn, segmentation, segmentation:frames, segmentation:full, segmentation:legs, segmentation:lenses, segmentation:shadows, segmentation:smart)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"!ls -R /kaggle/working/gd_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:36:51.267214Z","iopub.execute_input":"2026-01-12T00:36:51.267899Z","iopub.status.idle":"2026-01-12T00:36:51.412886Z","shell.execute_reply.started":"2026-01-12T00:36:51.267864Z","shell.execute_reply":"2026-01-12T00:36:51.411989Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/gd_out:\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"OUT_DIR = \"/kaggle/working/gd_out\"\n\n# show everything that exists\n!echo \"== OUT_DIR tree ==\" \n!ls -lah \"$OUT_DIR\"\n!echo\n!echo \"== recursive listing (first 200 lines) ==\"\n!ls -R \"$OUT_DIR\" | head -n 200\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:38:20.949907Z","iopub.execute_input":"2026-01-12T00:38:20.950202Z","iopub.status.idle":"2026-01-12T00:38:21.651281Z","shell.execute_reply.started":"2026-01-12T00:38:20.950178Z","shell.execute_reply":"2026-01-12T00:38:21.650512Z"}},"outputs":[{"name":"stdout","text":"== OUT_DIR tree ==\ntotal 8.0K\ndrwxr-xr-x 2 root root 4.0K Jan 12 00:36 .\ndrwxr-xr-x 5 root root 4.0K Jan 12 00:36 ..\n\n== recursive listing (first 200 lines) ==\n/kaggle/working/gd_out:\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"OUT_CSV = \"/kaggle/working/glasses_preds.csv\"\n\n# Important: output is a FILE (.csv), not a directory.\n# Important: task is classification:anyglasses (default per docs), size small (weights available)\n!glasses-detector -i \"$CROP_DIR\" -o \"$OUT_CSV\" -t \"classification:anyglasses\" -s small -f proba\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:41:08.559206Z","iopub.execute_input":"2026-01-12T00:41:08.559827Z","iopub.status.idle":"2026-01-12T00:43:37.115013Z","shell.execute_reply.started":"2026-01-12T00:41:08.559794Z","shell.execute_reply":"2026-01-12T00:43:37.114269Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://github.com/mantasu/glasses-detector/releases/download/v1.0.0/classification_anyglasses_tinyclsnet_v1.pth\" to /root/.cache/torch/hub/checkpoints/classification_anyglasses_tinyclsnet_v1.pth\n100%|█████████████████████████████████████████| 122k/122k [00:00<00:00, 650kB/s]\nProcessing: 100%|██████████████████████| 18149/18149 [02:23<00:00, 126.62file/s]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import os, pandas as pd\n\nprint(\"CSV exists:\", os.path.exists(\"/kaggle/working/glasses_preds.csv\"))\npreds = pd.read_csv(\"/kaggle/working/glasses_preds.csv\")\nprint(\"Columns:\", preds.columns.tolist())\npreds.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:43:59.912895Z","iopub.execute_input":"2026-01-12T00:43:59.913661Z","iopub.status.idle":"2026-01-12T00:43:59.948410Z","shell.execute_reply.started":"2026-01-12T00:43:59.913630Z","shell.execute_reply":"2026-01-12T00:43:59.947780Z"}},"outputs":[{"name":"stdout","text":"CSV exists: True\nColumns: ['crop_004435.jpg', '0.45941266417503357']\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"   crop_004435.jpg  0.45941266417503357\n0  crop_006553.jpg             0.000046\n1  crop_001994.jpg             0.000028\n2  crop_005715.jpg             0.946020\n3  crop_012864.jpg             0.836092\n4  crop_011280.jpg             0.000629","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>crop_004435.jpg</th>\n      <th>0.45941266417503357</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>crop_006553.jpg</td>\n      <td>0.000046</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crop_001994.jpg</td>\n      <td>0.000028</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>crop_005715.jpg</td>\n      <td>0.946020</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>crop_012864.jpg</td>\n      <td>0.836092</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>crop_011280.jpg</td>\n      <td>0.000629</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport numpy as np\n\nOUT_CSV = \"/kaggle/working/glasses_preds.csv\"\n\npreds = pd.read_csv(OUT_CSV, header=None, names=[\"crop_file\", \"score\"])\nprint(preds.head())\nprint(\"Rows:\", len(preds))\nprint(preds[\"score\"].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:46:47.073512Z","iopub.execute_input":"2026-01-12T00:46:47.074375Z","iopub.status.idle":"2026-01-12T00:46:47.100574Z","shell.execute_reply.started":"2026-01-12T00:46:47.074324Z","shell.execute_reply":"2026-01-12T00:46:47.099989Z"}},"outputs":[{"name":"stdout","text":"         crop_file     score\n0  crop_004435.jpg  0.459413\n1  crop_006553.jpg  0.000046\n2  crop_001994.jpg  0.000028\n3  crop_005715.jpg  0.946020\n4  crop_012864.jpg  0.836092\nRows: 18149\ncount    1.814900e+04\nmean     3.183762e-01\nstd      3.992660e-01\nmin      2.114917e-08\n25%      1.088421e-03\n50%      3.834856e-02\n75%      7.842783e-01\nmax      9.999256e-01\nName: score, dtype: float64\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"#Threshold + merge\n# choose a threshold\nTH = 0.50  # start with 0.50 (we can tune)\n\npreds[\"glasses\"] = (preds[\"score\"] >= TH).astype(int)\n\n# df_crops already has crop_path and orig_path\ndf_crops[\"crop_file\"] = df_crops[\"crop_path\"].apply(os.path.basename)\n\ndf_attr = df_crops.merge(preds[[\"crop_file\",\"score\",\"glasses\"]], on=\"crop_file\", how=\"inner\")\n\nprint(\"Merged:\", len(df_attr))\nprint(df_attr[\"glasses\"].value_counts())\ndf_attr.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:47:34.903421Z","iopub.execute_input":"2026-01-12T00:47:34.903977Z","iopub.status.idle":"2026-01-12T00:47:34.937610Z","shell.execute_reply.started":"2026-01-12T00:47:34.903950Z","shell.execute_reply":"2026-01-12T00:47:34.937046Z"}},"outputs":[{"name":"stdout","text":"Merged: 18149\nglasses\n0    12311\n1     5838\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"                                           orig_path  \\\n0  /kaggle/input/meglass/MeGlass_ori/51368783@N00...   \n1  /kaggle/input/meglass/MeGlass_ori/96841989@N00...   \n2  /kaggle/input/meglass/MeGlass_ori/10081417@N00...   \n3  /kaggle/input/meglass/MeGlass_ori/40286210@N00...   \n4  /kaggle/input/meglass/MeGlass_ori/16446760@N00...   \n\n                                    crop_path        crop_file     score  \\\n0  /kaggle/working/face_crops/crop_000000.jpg  crop_000000.jpg  0.988419   \n1  /kaggle/working/face_crops/crop_000001.jpg  crop_000001.jpg  0.889789   \n2  /kaggle/working/face_crops/crop_000002.jpg  crop_000002.jpg  0.044131   \n3  /kaggle/working/face_crops/crop_000003.jpg  crop_000003.jpg  0.000007   \n4  /kaggle/working/face_crops/crop_000004.jpg  crop_000004.jpg  0.671037   \n\n   glasses  \n0        1  \n1        1  \n2        0  \n3        0  \n4        1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>orig_path</th>\n      <th>crop_path</th>\n      <th>crop_file</th>\n      <th>score</th>\n      <th>glasses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/51368783@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000000.jpg</td>\n      <td>crop_000000.jpg</td>\n      <td>0.988419</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/96841989@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000001.jpg</td>\n      <td>crop_000001.jpg</td>\n      <td>0.889789</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/10081417@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000002.jpg</td>\n      <td>crop_000002.jpg</td>\n      <td>0.044131</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/40286210@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000003.jpg</td>\n      <td>crop_000003.jpg</td>\n      <td>0.000007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/16446760@N00...</td>\n      <td>/kaggle/working/face_crops/crop_000004.jpg</td>\n      <td>crop_000004.jpg</td>\n      <td>0.671037</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"#High-confidence cohorts\nCONF = 0.90  # confidence filter\ndf_hc = df_attr[(df_attr[\"score\"] >= CONF) | (df_attr[\"score\"] <= 1-CONF)].copy()\n\nprint(\"High-confidence kept:\", len(df_hc))\nprint(df_hc[\"glasses\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:48:00.730539Z","iopub.execute_input":"2026-01-12T00:48:00.731061Z","iopub.status.idle":"2026-01-12T00:48:00.739952Z","shell.execute_reply.started":"2026-01-12T00:48:00.731033Z","shell.execute_reply":"2026-01-12T00:48:00.739306Z"}},"outputs":[{"name":"stdout","text":"High-confidence kept: 13542\nglasses\n0    10185\n1     3357\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"#Sample 1000 eyeglasses + 1000 non-eyeglasses\nSEED = 42\nN_PER_GROUP = 1000\n\ndf_g = df_hc[df_hc[\"glasses\"]==1]\ndf_n = df_hc[df_hc[\"glasses\"]==0]\n\nprint(\"Available glasses:\", len(df_g), \"no-glasses:\", len(df_n))\n\nassert len(df_g) >= N_PER_GROUP and len(df_n) >= N_PER_GROUP, (\n    \"Not enough high-confidence samples. Lower CONF (e.g., 0.85) or increase MAX_SCAN.\"\n)\n\ndf_bal = pd.concat([\n    df_g.sample(N_PER_GROUP, random_state=SEED),\n    df_n.sample(N_PER_GROUP, random_state=SEED),\n], ignore_index=True).sample(frac=1, random_state=SEED).reset_index(drop=True)\n\n# final set for latency experiment\ndf_bal = df_bal.rename(columns={\"orig_path\":\"path\"})\nprint(df_bal[\"glasses\"].value_counts())\ndf_bal.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:48:43.966627Z","iopub.execute_input":"2026-01-12T00:48:43.967272Z","iopub.status.idle":"2026-01-12T00:48:43.986149Z","shell.execute_reply.started":"2026-01-12T00:48:43.967218Z","shell.execute_reply":"2026-01-12T00:48:43.985604Z"}},"outputs":[{"name":"stdout","text":"Available glasses: 3357 no-glasses: 10185\nglasses\n0    1000\n1    1000\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                                                path  \\\n0  /kaggle/input/meglass/MeGlass_ori/24133132@N04...   \n1  /kaggle/input/meglass/MeGlass_ori/98679389@N00...   \n2  /kaggle/input/meglass/MeGlass_ori/85772145@N00...   \n3  /kaggle/input/meglass/MeGlass_ori/25045012@N07...   \n4  /kaggle/input/meglass/MeGlass_ori/28196170@N02...   \n\n                                    crop_path        crop_file     score  \\\n0  /kaggle/working/face_crops/crop_005595.jpg  crop_005595.jpg  0.001448   \n1  /kaggle/working/face_crops/crop_011708.jpg  crop_011708.jpg  0.944852   \n2  /kaggle/working/face_crops/crop_002777.jpg  crop_002777.jpg  0.002165   \n3  /kaggle/working/face_crops/crop_014168.jpg  crop_014168.jpg  0.907228   \n4  /kaggle/working/face_crops/crop_015930.jpg  crop_015930.jpg  0.095365   \n\n   glasses  \n0        0  \n1        1  \n2        0  \n3        1  \n4        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>crop_path</th>\n      <th>crop_file</th>\n      <th>score</th>\n      <th>glasses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/24133132@N04...</td>\n      <td>/kaggle/working/face_crops/crop_005595.jpg</td>\n      <td>crop_005595.jpg</td>\n      <td>0.001448</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/98679389@N00...</td>\n      <td>/kaggle/working/face_crops/crop_011708.jpg</td>\n      <td>crop_011708.jpg</td>\n      <td>0.944852</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/85772145@N00...</td>\n      <td>/kaggle/working/face_crops/crop_002777.jpg</td>\n      <td>crop_002777.jpg</td>\n      <td>0.002165</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/25045012@N07...</td>\n      <td>/kaggle/working/face_crops/crop_014168.jpg</td>\n      <td>crop_014168.jpg</td>\n      <td>0.907228</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/28196170@N02...</td>\n      <td>/kaggle/working/face_crops/crop_015930.jpg</td>\n      <td>crop_015930.jpg</td>\n      <td>0.095365</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"print(df_bal[\"glasses\"].value_counts())\nprint(df_bal[[\"score\"]].describe())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:49:09.502782Z","iopub.execute_input":"2026-01-12T00:49:09.503545Z","iopub.status.idle":"2026-01-12T00:49:09.514515Z","shell.execute_reply.started":"2026-01-12T00:49:09.503515Z","shell.execute_reply":"2026-01-12T00:49:09.513621Z"}},"outputs":[{"name":"stdout","text":"glasses\n0    1000\n1    1000\nName: count, dtype: int64\n              score\ncount  2.000000e+03\nmean   4.870245e-01\nstd    4.764642e-01\nmin    1.207772e-07\n25%    1.672709e-03\n50%    4.982555e-01\n75%    9.683778e-01\nmax    9.999256e-01\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"#Latency Measurement (Rigorous & Reproducible)\n#warm up\nimport cv2, time\nfrom tqdm.auto import tqdm\n\n# Warm-up to stabilize GPU / kernels\nfor p in df_bal.head(30)[\"path\"]:\n    img = cv2.imread(p)\n    _ = app.get(img)\n\nprint(\"Warm-up completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:50:38.584874Z","iopub.execute_input":"2026-01-12T00:50:38.585195Z","iopub.status.idle":"2026-01-12T00:50:40.062809Z","shell.execute_reply.started":"2026-01-12T00:50:38.585167Z","shell.execute_reply":"2026-01-12T00:50:40.062107Z"}},"outputs":[{"name":"stdout","text":"Warm-up completed.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"#Robust latency measurement (K repeated runs)\nimport numpy as np\nimport pandas as pd\n\ndef trimmed_mean(arr, trim=0.2):\n    arr = np.sort(np.asarray(arr))\n    k = int(len(arr) * trim)\n    if len(arr) - 2*k <= 0:\n        return float(arr.mean())\n    return float(arr[k:len(arr)-k].mean())\n\ndef measure_latency(df, K=5):\n    records = []\n    for row in tqdm(df.itertuples(index=False), total=len(df)):\n        img = cv2.imread(row.path)\n\n        lats = []\n        faces_last = None\n        for _ in range(K):\n            t0 = time.perf_counter()\n            faces_last = app.get(img)\n            t1 = time.perf_counter()\n            lats.append((t1 - t0) * 1000)\n\n        n_faces = len(faces_last) if faces_last is not None else 0\n        det_score = (\n            float(getattr(faces_last[0], \"det_score\", np.nan))\n            if n_faces > 0 else np.nan\n        )\n\n        records.append({\n            \"path\": row.path,\n            \"glasses\": int(row.glasses),\n            \"lat_trim_ms\": trimmed_mean(lats, trim=0.2),\n            \"lat_p95_ms\": float(np.percentile(lats, 95)),\n            \"n_faces\": n_faces,\n            \"det_score\": det_score,\n            \"attr_score\": float(row.score),\n        })\n    return pd.DataFrame(records)\n\ndf_lat = measure_latency(df_bal, K=5)\ndf_lat.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:51:07.419634Z","iopub.execute_input":"2026-01-12T00:51:07.420354Z","iopub.status.idle":"2026-01-12T00:57:31.456362Z","shell.execute_reply.started":"2026-01-12T00:51:07.420321Z","shell.execute_reply":"2026-01-12T00:57:31.455689Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a470346a49e4cb3af82ebf2d26e7462"}},"metadata":{}},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"                                                path  glasses  lat_trim_ms  \\\n0  /kaggle/input/meglass/MeGlass_ori/24133132@N04...        0    31.609317   \n1  /kaggle/input/meglass/MeGlass_ori/98679389@N00...        1    30.712566   \n2  /kaggle/input/meglass/MeGlass_ori/85772145@N00...        0    29.580962   \n3  /kaggle/input/meglass/MeGlass_ori/25045012@N07...        1    30.883946   \n4  /kaggle/input/meglass/MeGlass_ori/28196170@N02...        0    48.928198   \n\n   lat_p95_ms  n_faces  det_score  attr_score  \n0   33.170164        1   0.887747    0.001448  \n1   32.061102        1   0.810409    0.944852  \n2   30.161001        1   0.834965    0.002165  \n3   31.503068        1   0.867405    0.907228  \n4   49.354776        2   0.817961    0.095365  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>glasses</th>\n      <th>lat_trim_ms</th>\n      <th>lat_p95_ms</th>\n      <th>n_faces</th>\n      <th>det_score</th>\n      <th>attr_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/24133132@N04...</td>\n      <td>0</td>\n      <td>31.609317</td>\n      <td>33.170164</td>\n      <td>1</td>\n      <td>0.887747</td>\n      <td>0.001448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/98679389@N00...</td>\n      <td>1</td>\n      <td>30.712566</td>\n      <td>32.061102</td>\n      <td>1</td>\n      <td>0.810409</td>\n      <td>0.944852</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/85772145@N00...</td>\n      <td>0</td>\n      <td>29.580962</td>\n      <td>30.161001</td>\n      <td>1</td>\n      <td>0.834965</td>\n      <td>0.002165</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/25045012@N07...</td>\n      <td>1</td>\n      <td>30.883946</td>\n      <td>31.503068</td>\n      <td>1</td>\n      <td>0.867405</td>\n      <td>0.907228</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/28196170@N02...</td>\n      <td>0</td>\n      <td>48.928198</td>\n      <td>49.354776</td>\n      <td>2</td>\n      <td>0.817961</td>\n      <td>0.095365</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"#Fairness metrics (mean, p95, p99)\n#Latency Fairness Metrics (Mean + Tail)\n#This is where novelty starts\ndef pct(x, p):\n    return float(np.percentile(np.asarray(x), p))\n\ndef latency_report(df):\n    rows = []\n    for lab, name in [(1,\"Eyeglasses\"), (0,\"No-Eyeglasses\")]:\n        x = df[df[\"glasses\"]==lab][\"lat_trim_ms\"].values\n        rows.append({\n            \"group\": name,\n            \"n\": len(x),\n            \"mean_ms\": x.mean(),\n            \"median_ms\": np.median(x),\n            \"p95_ms\": pct(x,95),\n            \"p99_ms\": pct(x,99),\n        })\n    rep = pd.DataFrame(rows)\n\n    g = rep[rep[\"group\"]==\"Eyeglasses\"].iloc[0]\n    n = rep[rep[\"group\"]==\"No-Eyeglasses\"].iloc[0]\n\n    fairness = {\n        \"mean_gap_ms\": g[\"mean_ms\"] - n[\"mean_ms\"],\n        \"mean_ratio\": g[\"mean_ms\"] / n[\"mean_ms\"],\n        \"p95_gap_ms\": g[\"p95_ms\"] - n[\"p95_ms\"],\n        \"p99_gap_ms\": g[\"p99_ms\"] - n[\"p99_ms\"],\n    }\n    return rep, fairness\n\nrep, fairness = latency_report(df_lat)\nrep, fairness\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T00:59:29.864106Z","iopub.execute_input":"2026-01-12T00:59:29.864654Z","iopub.status.idle":"2026-01-12T00:59:29.879465Z","shell.execute_reply.started":"2026-01-12T00:59:29.864624Z","shell.execute_reply":"2026-01-12T00:59:29.878765Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"(           group     n    mean_ms  median_ms     p95_ms     p99_ms\n 0     Eyeglasses  1000  36.190637  30.859949  51.263188  87.003824\n 1  No-Eyeglasses  1000  35.682493  30.815265  50.752241  69.121282,\n {'mean_gap_ms': np.float64(0.5081433719851631),\n  'mean_ratio': np.float64(1.0142406913549404),\n  'p95_gap_ms': np.float64(0.5109471501631049),\n  'p99_gap_ms': np.float64(17.882541609551495)})"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"#Statistical Evidence\n#Effect sizes\ndef cohens_d(a, b):\n    a, b = np.asarray(a), np.asarray(b)\n    sp = np.sqrt(((a.var(ddof=1)*(len(a)-1)) + (b.var(ddof=1)*(len(b)-1))) / (len(a)+len(b)-2))\n    return (a.mean() - b.mean()) / sp\n\ndef cliffs_delta(a, b):\n    a, b = np.asarray(a), np.asarray(b)\n    return (np.sum(a[:,None] > b) - np.sum(a[:,None] < b)) / (len(a)*len(b))\n\ng = df_lat[df_lat[\"glasses\"]==1][\"lat_trim_ms\"]\nn = df_lat[df_lat[\"glasses\"]==0][\"lat_trim_ms\"]\n\nprint(\"Cohen’s d:\", cohens_d(g, n))\nprint(\"Cliff’s delta:\", cliffs_delta(g, n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:00:29.712992Z","iopub.execute_input":"2026-01-12T01:00:29.713281Z","iopub.status.idle":"2026-01-12T01:00:29.725677Z","shell.execute_reply.started":"2026-01-12T01:00:29.713258Z","shell.execute_reply":"2026-01-12T01:00:29.725013Z"}},"outputs":[{"name":"stdout","text":"Cohen’s d: 0.040919269842809664\nCliff’s delta: 0.032832\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"#Permutation test (distribution-free)\ndef permutation_test(a, b, n_perm=3000, seed=42):\n    rng = np.random.default_rng(seed)\n    obs = a.mean() - b.mean()\n    pooled = np.concatenate([a,b])\n    cnt = 0\n    for _ in range(n_perm):\n        rng.shuffle(pooled)\n        if abs(pooled[:len(a)].mean() - pooled[len(a):].mean()) >= abs(obs):\n            cnt += 1\n    return obs, (cnt+1)/(n_perm+1)\n\nobs, p = permutation_test(g.values, n.values)\nprint(\"Observed mean diff (ms):\", obs)\nprint(\"Permutation p-value:\", p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:01:10.916511Z","iopub.execute_input":"2026-01-12T01:01:10.917262Z","iopub.status.idle":"2026-01-12T01:01:11.026387Z","shell.execute_reply.started":"2026-01-12T01:01:10.917212Z","shell.execute_reply":"2026-01-12T01:01:11.025742Z"}},"outputs":[{"name":"stdout","text":"Observed mean diff (ms): 0.5081433719851631\nPermutation p-value: 0.3598800399866711\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"#Bootstrap confidence interval\ndef bootstrap_ci(a, b, n_boot=3000, seed=7):\n    rng = np.random.default_rng(seed)\n    diffs = []\n    for _ in range(n_boot):\n        diffs.append(\n            rng.choice(a, len(a), True).mean() -\n            rng.choice(b, len(b), True).mean()\n        )\n    return np.percentile(diffs, [2.5, 97.5])\n\nprint(\"95% CI (ms):\", bootstrap_ci(g.values, n.values))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:01:53.979926Z","iopub.execute_input":"2026-01-12T01:01:53.980231Z","iopub.status.idle":"2026-01-12T01:01:54.129300Z","shell.execute_reply.started":"2026-01-12T01:01:53.980204Z","shell.execute_reply":"2026-01-12T01:01:54.128566Z"}},"outputs":[{"name":"stdout","text":"95% CI (ms): [-0.56742679  1.59402034]\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"#Controlled Analysis (Responsible AI)\n#Regression with controls\nfrom sklearn.linear_model import LinearRegression\n\ntmp = df_lat.dropna(subset=[\"lat_trim_ms\",\"det_score\"]).copy()\ntmp[\"is_glasses\"] = (tmp[\"glasses\"]==1).astype(int)\n\nX = tmp[[\"is_glasses\",\"n_faces\",\"det_score\",\"attr_score\"]].values\ny = tmp[\"lat_trim_ms\"].values\n\nlr = LinearRegression().fit(X, y)\n\nprint(\"Coefficients [is_glasses, n_faces, det_score, attr_score]:\")\nprint(lr.coef_)\nprint(\"R²:\", lr.score(X, y))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:03:00.874409Z","iopub.execute_input":"2026-01-12T01:03:00.874702Z","iopub.status.idle":"2026-01-12T01:03:01.426692Z","shell.execute_reply.started":"2026-01-12T01:03:00.874677Z","shell.execute_reply":"2026-01-12T01:03:01.425903Z"}},"outputs":[{"name":"stdout","text":"Coefficients [is_glasses, n_faces, det_score, attr_score]:\n[-2.46066567e+00  1.83840276e+01 -1.44717809e-02  2.65667839e+00]\nR²: 0.9955805196525231\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LinearRegression\n\ntmp = df_lat.dropna(subset=[\"lat_trim_ms\",\"det_score\"]).copy()\ntmp[\"is_glasses\"] = (tmp[\"glasses\"]==1).astype(int)\n\nX = tmp[[\"is_glasses\",\"n_faces\",\"det_score\",\"attr_score\"]].values\ny = tmp[\"lat_trim_ms\"].values\n\nlr = LinearRegression().fit(X, y)\n\nprint(\"Coefficients [is_glasses, n_faces, det_score, attr_score]:\")\nprint(lr.coef_)\nprint(\"R²:\", lr.score(X, y))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Mitigation Challenges\n#Smaller detector resolution\nfrom insightface.app import FaceAnalysis\n\ndef build_app(det_size):\n    a = FaceAnalysis(name=\"buffalo_l\",\n                     providers=[\"CUDAExecutionProvider\",\"CPUExecutionProvider\"])\n    a.prepare(ctx_id=0, det_size=det_size)\n    for p in df_bal.head(20)[\"path\"]:\n        _ = a.get(cv2.imread(p))\n    return a\n\napp_fast = build_app((320,320))\n\ndef quick_latency(df, face_app):\n    out=[]\n    for r in df.itertuples(index=False):\n        img=cv2.imread(r.path)\n        t0=time.perf_counter()\n        _=face_app.get(img)\n        t1=time.perf_counter()\n        out.append({\"glasses\":r.glasses,\"lat_ms\":(t1-t0)*1000})\n    return pd.DataFrame(out)\n\ndf_320 = quick_latency(df_bal, app_fast)\nrep_320, fair_320 = latency_report(df_320.rename(columns={\"lat_ms\":\"lat_trim_ms\"}))\nrep_320, fair_320\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:06:43.545307Z","iopub.execute_input":"2026-01-12T01:06:43.546547Z","iopub.status.idle":"2026-01-12T01:08:02.944034Z","shell.execute_reply.started":"2026-01-12T01:06:43.546501Z","shell.execute_reply":"2026-01-12T01:08:02.943121Z"}},"outputs":[{"name":"stdout","text":"Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\nset det-size: (320, 320)\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(           group     n    mean_ms  median_ms     p95_ms     p99_ms\n 0     Eyeglasses  1000  28.822150  24.205348  43.794876  62.336772\n 1  No-Eyeglasses  1000  28.536558  24.113008  43.552583  61.572876,\n {'mean_gap_ms': np.float64(0.28559216000212473),\n  'mean_ratio': np.float64(1.010007940160688),\n  'p95_gap_ms': np.float64(0.2422925002520131),\n  'p99_gap_ms': np.float64(0.7638963098450873)})"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"#Fairness-aware dynamic fallapp_acc = app  # original (640x640)\n\ndef dynamic_policy(df, score_th=0.35):\n    out=[]\n    for r in df.itertuples(index=False):\n        img=cv2.imread(r.path)\n        t0=time.perf_counter()\n        faces=app_fast.get(img)\n        if len(faces)==0 or float(getattr(faces[0],\"det_score\",0))<score_th:\n            faces=app_acc.get(img)\n        t1=time.perf_counter()\n        out.append({\"glasses\":r.glasses,\"lat_ms\":(t1-t0)*1000})\n    return pd.DataFrame(out)\n\ndf_dyn = dynamic_policy(df_bal)\nrep_dyn, fair_dyn = latency_report(df_dyn.rename(columns={\"lat_ms\":\"lat_trim_ms\"}))\nrep_dyn, fair_dyn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:09:19.833335Z","iopub.execute_input":"2026-01-12T01:09:19.833984Z","iopub.status.idle":"2026-01-12T01:10:36.791323Z","shell.execute_reply.started":"2026-01-12T01:09:19.833934Z","shell.execute_reply":"2026-01-12T01:10:36.790465Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(           group     n    mean_ms  median_ms     p95_ms     p99_ms\n 0     Eyeglasses  1000  28.855279  24.226732  43.870719  63.279570\n 1  No-Eyeglasses  1000  28.557460  24.147345  43.396380  61.596625,\n {'mean_gap_ms': np.float64(0.2978191010224691),\n  'mean_ratio': np.float64(1.0104287672937073),\n  'p95_gap_ms': np.float64(0.47433824943254876),\n  'p99_gap_ms': np.float64(1.6829449597388262)})"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"#measured dynamic policy\ndef dynamic_policy_repeated(df, score_th=0.35, K=5):\n    out=[]\n    for r in tqdm(df.itertuples(index=False), total=len(df)):\n        img=cv2.imread(r.path)\n\n        lats=[]\n        fb=0\n        for _ in range(K):\n            t0=time.perf_counter()\n            faces=app_fast.get(img)\n            used_fb = (len(faces)==0) or (float(getattr(faces[0],\"det_score\",0)) < score_th)\n            if used_fb:\n                fb += 1\n                faces=app_acc.get(img)\n            t1=time.perf_counter()\n            lats.append((t1-t0)*1000)\n\n        out.append({\n            \"glasses\": int(r.glasses),\n            \"lat_trim_ms\": trimmed_mean(lats, 0.2),\n            \"fallback_rate\": fb / K\n        })\n    return pd.DataFrame(out)\n\ndf_dyn2 = dynamic_policy_repeated(df_bal, score_th=0.35, K=5)\nrep_dyn2, fair_dyn2 = latency_report(df_dyn2)\nprint(rep_dyn2)\nprint(fair_dyn2)\n\n# explain \"why\": compare fallback rates\nprint(\"\\nFallback rate by group:\")\nprint(df_dyn2.groupby(\"glasses\")[\"fallback_rate\"].mean())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:15:51.783565Z","iopub.execute_input":"2026-01-12T01:15:51.784154Z","iopub.status.idle":"2026-01-12T01:20:57.283580Z","shell.execute_reply.started":"2026-01-12T01:15:51.784124Z","shell.execute_reply":"2026-01-12T01:20:57.282964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46046a5e135246e184157500daacd197"}},"metadata":{}},{"name":"stdout","text":"           group     n    mean_ms  median_ms     p95_ms     p99_ms\n0     Eyeglasses  1000  28.483209  23.864078  42.971275  63.863574\n1  No-Eyeglasses  1000  28.221585  23.812915  42.931242  60.921359\n{'mean_gap_ms': np.float64(0.2616234823296182), 'mean_ratio': np.float64(1.0092703325850148), 'p95_gap_ms': np.float64(0.04003256626674556), 'p99_gap_ms': np.float64(2.9422152603727767)}\n\nFallback rate by group:\nglasses\n0    0.0\n1    0.0\nName: fallback_rate, dtype: float64\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"#best threshold\nths = [0.20, 0.30, 0.35, 0.40, 0.50]\nrows=[]\nfor th in ths:\n    df_tmp = dynamic_policy_repeated(df_bal, score_th=th, K=3)  # use K=3 for sweep\n    rep, fair = latency_report(df_tmp)\n    rows.append({\n        \"score_th\": th,\n        \"mean_gap_ms\": float(fair[\"mean_gap_ms\"]),\n        \"p99_gap_ms\": float(fair[\"p99_gap_ms\"]),\n        \"overall_mean_ms\": float(df_tmp[\"lat_trim_ms\"].mean()),\n        \"fallback_rate_mean\": float(df_tmp[\"fallback_rate\"].mean()),\n        \"fallback_rate_glasses\": float(df_tmp[df_tmp[\"glasses\"]==1][\"fallback_rate\"].mean()),\n        \"fallback_rate_noglasses\": float(df_tmp[df_tmp[\"glasses\"]==0][\"fallback_rate\"].mean()),\n    })\npd.DataFrame(rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:22:07.448102Z","iopub.execute_input":"2026-01-12T01:22:07.448889Z","iopub.status.idle":"2026-01-12T01:38:05.361837Z","shell.execute_reply.started":"2026-01-12T01:22:07.448856Z","shell.execute_reply":"2026-01-12T01:38:05.361281Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fb0b06dff384ad59cf35d50d97ab288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f67c9053584909947883c08fde89b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89286842d3fc47579603130559cc2b49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74225fe7580f47a1b35c9b7ea816f6a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd34d183fb2f4c10a0b73e941f53d3a4"}},"metadata":{}},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"   score_th  mean_gap_ms  p99_gap_ms  overall_mean_ms  fallback_rate_mean  \\\n0      0.20     0.266743    1.717766        28.476903                 0.0   \n1      0.30     0.222942    1.912234        28.483747                 0.0   \n2      0.35     0.278805    1.442371        28.454042                 0.0   \n3      0.40     0.299936    3.052975        28.516791                 0.0   \n4      0.50     0.213999    1.853310        28.399497                 0.0   \n\n   fallback_rate_glasses  fallback_rate_noglasses  \n0                    0.0                      0.0  \n1                    0.0                      0.0  \n2                    0.0                      0.0  \n3                    0.0                      0.0  \n4                    0.0                      0.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score_th</th>\n      <th>mean_gap_ms</th>\n      <th>p99_gap_ms</th>\n      <th>overall_mean_ms</th>\n      <th>fallback_rate_mean</th>\n      <th>fallback_rate_glasses</th>\n      <th>fallback_rate_noglasses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.20</td>\n      <td>0.266743</td>\n      <td>1.717766</td>\n      <td>28.476903</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.30</td>\n      <td>0.222942</td>\n      <td>1.912234</td>\n      <td>28.483747</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.35</td>\n      <td>0.278805</td>\n      <td>1.442371</td>\n      <td>28.454042</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.40</td>\n      <td>0.299936</td>\n      <td>3.052975</td>\n      <td>28.516791</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.50</td>\n      <td>0.213999</td>\n      <td>1.853310</td>\n      <td>28.399497</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"#Confidence Filtering Ablation (CONF = 0.85, 0.90, 0.95)\nimport numpy as np\nimport pandas as pd\n\nSEED = 42\nN_PER_GROUP = 1000\n\ndef build_balanced_cohort(df_attr, conf=0.90, n_per_group=1000, seed=42):\n    \"\"\"\n    df_attr columns expected:\n      - orig_path (or path)\n      - glasses (0/1)\n      - score (probability of glasses)\n    \"\"\"\n    df = df_attr.copy()\n\n    # High-confidence filter (keep near 0 or near 1)\n    df_hc = df[(df[\"score\"] >= conf) | (df[\"score\"] <= 1-conf)].copy()\n\n    df_g = df_hc[df_hc[\"glasses\"] == 1]\n    df_n = df_hc[df_hc[\"glasses\"] == 0]\n\n    if len(df_g) < n_per_group or len(df_n) < n_per_group:\n        return None, {\"conf\": conf, \"available_glasses\": len(df_g), \"available_noglasses\": len(df_n)}\n\n    df_bal = pd.concat([\n        df_g.sample(n_per_group, random_state=seed),\n        df_n.sample(n_per_group, random_state=seed),\n    ], ignore_index=True).sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    # standardize to a \"path\" column for latency measurement\n    if \"orig_path\" in df_bal.columns:\n        df_bal = df_bal.rename(columns={\"orig_path\": \"path\"})\n    elif \"path\" not in df_bal.columns:\n        raise ValueError(\"df_attr must contain orig_path or path column.\")\n\n    meta = {\"conf\": conf, \"available_glasses\": len(df_g), \"available_noglasses\": len(df_n)}\n    return df_bal[[\"path\", \"glasses\", \"score\"]].copy(), meta\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:48:24.833493Z","iopub.execute_input":"2026-01-12T01:48:24.834126Z","iopub.status.idle":"2026-01-12T01:48:24.840783Z","shell.execute_reply.started":"2026-01-12T01:48:24.834088Z","shell.execute_reply":"2026-01-12T01:48:24.840020Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"#latency measurement\nimport cv2, time\nfrom tqdm.auto import tqdm\n\ndef trimmed_mean(arr, trim=0.2):\n    arr = np.sort(np.asarray(arr))\n    k = int(len(arr) * trim)\n    if len(arr) - 2*k <= 0:\n        return float(arr.mean())\n    return float(arr[k:len(arr)-k].mean())\n\ndef measure_latency(df, app, K=3):\n    recs = []\n    for r in tqdm(df.itertuples(index=False), total=len(df)):\n        img = cv2.imread(r.path)\n        lats = []\n        faces_last = None\n        for _ in range(K):\n            t0 = time.perf_counter()\n            faces_last = app.get(img)\n            t1 = time.perf_counter()\n            lats.append((t1 - t0) * 1000.0)\n\n        n_faces = len(faces_last) if faces_last is not None else 0\n        det_score = float(getattr(faces_last[0], \"det_score\", np.nan)) if n_faces > 0 else np.nan\n\n        recs.append({\n            \"path\": r.path,\n            \"glasses\": int(r.glasses),\n            \"lat_trim_ms\": trimmed_mean(lats, 0.2),\n            \"lat_p99_ms\": float(np.percentile(lats, 99)),\n            \"n_faces\": n_faces,\n            \"det_score\": det_score,\n            \"attr_score\": float(r.score),\n        })\n    return pd.DataFrame(recs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:48:47.455647Z","iopub.execute_input":"2026-01-12T01:48:47.456395Z","iopub.status.idle":"2026-01-12T01:48:47.463182Z","shell.execute_reply.started":"2026-01-12T01:48:47.456364Z","shell.execute_reply":"2026-01-12T01:48:47.462388Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"#fairness report\ndef pct(x, p): \n    return float(np.percentile(np.asarray(x), p))\n\ndef latency_fairness(df):\n    out = {}\n    g = df[df[\"glasses\"]==1][\"lat_trim_ms\"].values\n    n = df[df[\"glasses\"]==0][\"lat_trim_ms\"].values\n\n    out[\"mean_glasses_ms\"] = float(np.mean(g))\n    out[\"mean_noglasses_ms\"] = float(np.mean(n))\n    out[\"p99_glasses_ms\"] = pct(g, 99)\n    out[\"p99_noglasses_ms\"] = pct(n, 99)\n\n    out[\"mean_gap_ms\"] = out[\"mean_glasses_ms\"] - out[\"mean_noglasses_ms\"]\n    out[\"mean_ratio\"] = out[\"mean_glasses_ms\"] / out[\"mean_noglasses_ms\"]\n    out[\"p99_gap_ms\"] = out[\"p99_glasses_ms\"] - out[\"p99_noglasses_ms\"]\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:49:14.899599Z","iopub.execute_input":"2026-01-12T01:49:14.900102Z","iopub.status.idle":"2026-01-12T01:49:14.905441Z","shell.execute_reply.started":"2026-01-12T01:49:14.900077Z","shell.execute_reply":"2026-01-12T01:49:14.904685Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"#Run the CONF ablation\n# Warm-up once (stabilizes GPU timing)\nfor p in df_attr.sample(20, random_state=SEED)[\"orig_path\"]:\n    _ = app.get(cv2.imread(p))\n\nCONF_LIST = [0.85, 0.90, 0.95]\nrows = []\n\nfor conf in CONF_LIST:\n    df_bal_conf, meta = build_balanced_cohort(df_attr, conf=conf, n_per_group=N_PER_GROUP, seed=SEED)\n\n    if df_bal_conf is None:\n        rows.append({\n            \"CONF\": conf,\n            \"status\": \"insufficient samples\",\n            **meta\n        })\n        continue\n\n    df_lat_conf = measure_latency(df_bal_conf, app, K=3)\n    fair = latency_fairness(df_lat_conf)\n\n    rows.append({\n        \"CONF\": conf,\n        \"status\": \"ok\",\n        **meta,\n        \"mean_gap_ms\": fair[\"mean_gap_ms\"],\n        \"mean_ratio\": fair[\"mean_ratio\"],\n        \"p99_gap_ms\": fair[\"p99_gap_ms\"],\n        \"mean_glasses_ms\": fair[\"mean_glasses_ms\"],\n        \"mean_noglasses_ms\": fair[\"mean_noglasses_ms\"],\n    })\n\nablation_table = pd.DataFrame(rows)\nablation_table\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T01:49:40.659497Z","iopub.execute_input":"2026-01-12T01:49:40.660022Z","iopub.status.idle":"2026-01-12T02:01:25.629823Z","shell.execute_reply.started":"2026-01-12T01:49:40.659993Z","shell.execute_reply":"2026-01-12T02:01:25.629224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fcc535f59594d2697bc3602d769b7b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca3a90e48ff400b833fb93ba1e81ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89518cb528594f3f933ae1d296c05d4b"}},"metadata":{}},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"   CONF status  conf  available_glasses  available_noglasses  mean_gap_ms  \\\n0  0.85     ok  0.85               4010                10646     1.200372   \n1  0.90     ok  0.90               3357                10185     0.528272   \n2  0.95     ok  0.95               2329                 9392     0.551966   \n\n   mean_ratio  p99_gap_ms  mean_glasses_ms  mean_noglasses_ms  \n0    1.034268   19.861827        36.229629          35.029257  \n1    1.015199   16.446067        35.285458          34.757186  \n2    1.015631   16.112175        35.863645          35.311679  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CONF</th>\n      <th>status</th>\n      <th>conf</th>\n      <th>available_glasses</th>\n      <th>available_noglasses</th>\n      <th>mean_gap_ms</th>\n      <th>mean_ratio</th>\n      <th>p99_gap_ms</th>\n      <th>mean_glasses_ms</th>\n      <th>mean_noglasses_ms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.85</td>\n      <td>ok</td>\n      <td>0.85</td>\n      <td>4010</td>\n      <td>10646</td>\n      <td>1.200372</td>\n      <td>1.034268</td>\n      <td>19.861827</td>\n      <td>36.229629</td>\n      <td>35.029257</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.90</td>\n      <td>ok</td>\n      <td>0.90</td>\n      <td>3357</td>\n      <td>10185</td>\n      <td>0.528272</td>\n      <td>1.015199</td>\n      <td>16.446067</td>\n      <td>35.285458</td>\n      <td>34.757186</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.95</td>\n      <td>ok</td>\n      <td>0.95</td>\n      <td>2329</td>\n      <td>9392</td>\n      <td>0.551966</td>\n      <td>1.015631</td>\n      <td>16.112175</td>\n      <td>35.863645</td>\n      <td>35.311679</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"#Visualization (CDF + Boxplot) for Latency\n#CDF plot (glasses vs no-glasses)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_latency_cdf(df, col=\"lat_trim_ms\", title=\"Latency CDF (Trimmed Mean)\"):\n    g = np.sort(df[df[\"glasses\"]==1][col].values)\n    n = np.sort(df[df[\"glasses\"]==0][col].values)\n\n    yg = np.linspace(0, 1, len(g), endpoint=True)\n    yn = np.linspace(0, 1, len(n), endpoint=True)\n\n    plt.figure(figsize=(7,5))\n    plt.plot(g, yg, label=\"Eyeglasses\")\n    plt.plot(n, yn, label=\"No-Eyeglasses\")\n    plt.xlabel(\"Latency (ms)\")\n    plt.ylabel(\"CDF\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Example usage:\n# plot_latency_cdf(df_lat, col=\"lat_trim_ms\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:06:42.002915Z","iopub.execute_input":"2026-01-12T02:06:42.003515Z","iopub.status.idle":"2026-01-12T02:06:42.008893Z","shell.execute_reply.started":"2026-01-12T02:06:42.003486Z","shell.execute_reply":"2026-01-12T02:06:42.008104Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"#Boxplot\nimport matplotlib.pyplot as plt\n\ndef boxplot_latency(df, col=\"lat_trim_ms\", title=\"Latency Distribution\"):\n    g = df[df[\"glasses\"]==1][col].values\n    n = df[df[\"glasses\"]==0][col].values\n\n    plt.figure(figsize=(6,4))\n    plt.boxplot([g, n], labels=[\"Eyeglasses\", \"No-Eyeglasses\"])\n    plt.ylabel(\"Latency (ms)\")\n    plt.title(title)\n    plt.grid(True, axis=\"y\", alpha=0.3)\n    plt.show()\n\n# Example usage:\n# boxplot_latency(df_lat, col=\"lat_trim_ms\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:06:44.935974Z","iopub.execute_input":"2026-01-12T02:06:44.936287Z","iopub.status.idle":"2026-01-12T02:06:44.941165Z","shell.execute_reply.started":"2026-01-12T02:06:44.936235Z","shell.execute_reply":"2026-01-12T02:06:44.940579Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"import pandas as pd\n\nprint(df_attr.columns)\n\n# Standardize original image path column to be called \"orig_path\"\nif \"orig_path\" not in df_attr.columns and \"path\" in df_attr.columns:\n    df_attr = df_attr.rename(columns={\"path\": \"orig_path\"})\n\nassert \"orig_path\" in df_attr.columns, \"df_attr must have orig_path or path\"\nassert \"glasses\" in df_attr.columns, \"df_attr must have glasses (0/1)\"\nassert \"score\" in df_attr.columns, \"df_attr must have score\"\n\ndf_attr[[\"orig_path\", \"glasses\", \"score\"]].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:11:03.072918Z","iopub.execute_input":"2026-01-12T02:11:03.073659Z","iopub.status.idle":"2026-01-12T02:11:03.085615Z","shell.execute_reply.started":"2026-01-12T02:11:03.073631Z","shell.execute_reply":"2026-01-12T02:11:03.085076Z"}},"outputs":[{"name":"stdout","text":"Index(['orig_path', 'crop_path', 'crop_file', 'score', 'glasses'], dtype='object')\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"                                           orig_path  glasses     score\n0  /kaggle/input/meglass/MeGlass_ori/51368783@N00...        1  0.988419\n1  /kaggle/input/meglass/MeGlass_ori/96841989@N00...        1  0.889789\n2  /kaggle/input/meglass/MeGlass_ori/10081417@N00...        0  0.044131\n3  /kaggle/input/meglass/MeGlass_ori/40286210@N00...        0  0.000007\n4  /kaggle/input/meglass/MeGlass_ori/16446760@N00...        1  0.671037","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>orig_path</th>\n      <th>glasses</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/51368783@N00...</td>\n      <td>1</td>\n      <td>0.988419</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/96841989@N00...</td>\n      <td>1</td>\n      <td>0.889789</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/10081417@N00...</td>\n      <td>0</td>\n      <td>0.044131</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/40286210@N00...</td>\n      <td>0</td>\n      <td>0.000007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/meglass/MeGlass_ori/16446760@N00...</td>\n      <td>1</td>\n      <td>0.671037</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"#Define helper functions (cohort builder + latency)\nimport numpy as np\nimport cv2, time\nfrom tqdm.auto import tqdm\n\nSEED = 42\nN_PER_GROUP = 1000\n\ndef trimmed_mean(arr, trim=0.2):\n    arr = np.sort(np.asarray(arr))\n    k = int(len(arr) * trim)\n    if len(arr) - 2*k <= 0:\n        return float(arr.mean())\n    return float(arr[k:len(arr)-k].mean())\n\ndef build_balanced_cohort(df_attr, CONF=0.90, n_per_group=1000, seed=42):\n    df = df_attr.copy()\n\n    # keep only high-confidence attribute predictions\n    df_hc = df[(df[\"score\"] >= CONF) | (df[\"score\"] <= 1-CONF)].copy()\n\n    df_g = df_hc[df_hc[\"glasses\"] == 1]\n    df_n = df_hc[df_hc[\"glasses\"] == 0]\n\n    if len(df_g) < n_per_group or len(df_n) < n_per_group:\n        return None, {\"CONF\": CONF, \"available_glasses\": len(df_g), \"available_noglasses\": len(df_n)}\n\n    df_bal = pd.concat([\n        df_g.sample(n_per_group, random_state=seed),\n        df_n.sample(n_per_group, random_state=seed),\n    ], ignore_index=True).sample(frac=1, random_state=seed).reset_index(drop=True)\n\n    df_bal = df_bal.rename(columns={\"orig_path\": \"path\"})\n    meta = {\"CONF\": CONF, \"available_glasses\": len(df_g), \"available_noglasses\": len(df_n)}\n    return df_bal[[\"path\", \"glasses\", \"score\"]].copy(), meta\n\ndef measure_latency(df_bal, app, K=3):\n    recs = []\n    for r in tqdm(df_bal.itertuples(index=False), total=len(df_bal)):\n        img = cv2.imread(r.path)\n\n        lats = []\n        faces_last = None\n        for _ in range(K):\n            t0 = time.perf_counter()\n            faces_last = app.get(img)\n            t1 = time.perf_counter()\n            lats.append((t1 - t0) * 1000)\n\n        n_faces = len(faces_last) if faces_last is not None else 0\n        det_score = float(getattr(faces_last[0], \"det_score\", np.nan)) if n_faces > 0 else np.nan\n\n        recs.append({\n            \"glasses\": int(r.glasses),\n            \"lat_trim_ms\": trimmed_mean(lats, 0.2),\n            \"det_score\": det_score,\n            \"attr_score\": float(r.score),\n            \"n_faces\": n_faces\n        })\n    return pd.DataFrame(recs)\n\ndef fairness_summary(df_lat):\n    g = df_lat[df_lat[\"glasses\"]==1][\"lat_trim_ms\"].values\n    n = df_lat[df_lat[\"glasses\"]==0][\"lat_trim_ms\"].values\n    return {\n        \"mean_gap_ms\": float(g.mean() - n.mean()),\n        \"mean_ratio\": float(g.mean() / n.mean()),\n        \"p95_gap_ms\": float(np.percentile(g,95) - np.percentile(n,95)),\n        \"p99_gap_ms\": float(np.percentile(g,99) - np.percentile(n,99)),\n        \"mean_glasses_ms\": float(g.mean()),\n        \"mean_noglasses_ms\": float(n.mean()),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:11:31.355890Z","iopub.execute_input":"2026-01-12T02:11:31.356599Z","iopub.status.idle":"2026-01-12T02:11:31.367543Z","shell.execute_reply.started":"2026-01-12T02:11:31.356571Z","shell.execute_reply":"2026-01-12T02:11:31.366741Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"#Warm-up (stabilizes timing)\n# warm up using random images\nwarm = df_attr.sample(20, random_state=SEED)[\"orig_path\"].tolist()\nfor p in warm:\n    img = cv2.imread(p)\n    _ = app.get(img)\n\nprint(\"Warm-up done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:11:53.311771Z","iopub.execute_input":"2026-01-12T02:11:53.312464Z","iopub.status.idle":"2026-01-12T02:11:54.234437Z","shell.execute_reply.started":"2026-01-12T02:11:53.312434Z","shell.execute_reply":"2026-01-12T02:11:54.233773Z"}},"outputs":[{"name":"stdout","text":"Warm-up done.\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"#Run the CONF ablation\nCONF_LIST = [0.85, 0.90, 0.95]\nrows = []\n\nfor CONF in CONF_LIST:\n    df_bal_conf, meta = build_balanced_cohort(df_attr, CONF=CONF, n_per_group=N_PER_GROUP, seed=SEED)\n\n    if df_bal_conf is None:\n        rows.append({**meta, \"status\": \"not enough samples\"})\n        continue\n\n    df_lat_conf = measure_latency(df_bal_conf, app, K=3)  # K=3 to keep it fast\n    fair = fairness_summary(df_lat_conf)\n\n    rows.append({**meta, \"status\": \"ok\", **fair})\n\nablation_table = pd.DataFrame(rows)\nablation_table\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:12:13.921885Z","iopub.execute_input":"2026-01-12T02:12:13.922180Z","iopub.status.idle":"2026-01-12T02:23:58.296823Z","shell.execute_reply.started":"2026-01-12T02:12:13.922152Z","shell.execute_reply":"2026-01-12T02:23:58.296196Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b688ecf7932437ea709421730d386f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73f7021871c14c0b9a254b8abae1f8c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2384e171c0c94a1e915cd8b36c025088"}},"metadata":{}},{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"   CONF  available_glasses  available_noglasses status  mean_gap_ms  \\\n0  0.85               4010                10646     ok     1.182861   \n1  0.90               3357                10185     ok     0.499388   \n2  0.95               2329                 9392     ok     0.537806   \n\n   mean_ratio  p95_gap_ms  p99_gap_ms  mean_glasses_ms  mean_noglasses_ms  \n0    1.033515    0.762645   21.018146        36.476843          35.293982  \n1    1.014245    0.202066   15.443715        35.555755          35.056367  \n2    1.015147    0.934963   17.857723        36.042634          35.504828  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CONF</th>\n      <th>available_glasses</th>\n      <th>available_noglasses</th>\n      <th>status</th>\n      <th>mean_gap_ms</th>\n      <th>mean_ratio</th>\n      <th>p95_gap_ms</th>\n      <th>p99_gap_ms</th>\n      <th>mean_glasses_ms</th>\n      <th>mean_noglasses_ms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.85</td>\n      <td>4010</td>\n      <td>10646</td>\n      <td>ok</td>\n      <td>1.182861</td>\n      <td>1.033515</td>\n      <td>0.762645</td>\n      <td>21.018146</td>\n      <td>36.476843</td>\n      <td>35.293982</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.90</td>\n      <td>3357</td>\n      <td>10185</td>\n      <td>ok</td>\n      <td>0.499388</td>\n      <td>1.014245</td>\n      <td>0.202066</td>\n      <td>15.443715</td>\n      <td>35.555755</td>\n      <td>35.056367</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.95</td>\n      <td>2329</td>\n      <td>9392</td>\n      <td>ok</td>\n      <td>0.537806</td>\n      <td>1.015147</td>\n      <td>0.934963</td>\n      <td>17.857723</td>\n      <td>36.042634</td>\n      <td>35.504828</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"#Pick the best CONF (usually 0.90) and run FINAL latency (K=5)\nBEST_CONF = 0.90  # choose from the table\n\ndf_bal_final, meta = build_balanced_cohort(df_attr, CONF=BEST_CONF, n_per_group=N_PER_GROUP, seed=SEED)\nprint(\"Using:\", meta)\n\ndf_lat_final = measure_latency(df_bal_final, app, K=5)\nfinal_fair = fairness_summary(df_lat_final)\n\nprint(\"FINAL fairness:\", final_fair)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:23:58.298154Z","iopub.execute_input":"2026-01-12T02:23:58.298501Z","iopub.status.idle":"2026-01-12T02:30:08.438530Z","shell.execute_reply.started":"2026-01-12T02:23:58.298468Z","shell.execute_reply":"2026-01-12T02:30:08.437901Z"}},"outputs":[{"name":"stdout","text":"Using: {'CONF': 0.9, 'available_glasses': 3357, 'available_noglasses': 10185}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7a8cadef0044f4bc15a33d1542f384"}},"metadata":{}},{"name":"stdout","text":"FINAL fairness: {'mean_gap_ms': 0.5174525536688961, 'mean_ratio': 1.014956719431222, 'p95_gap_ms': 0.46547068377548584, 'p99_gap_ms': 17.32276612670225, 'mean_glasses_ms': 35.114113676342946, 'mean_noglasses_ms': 34.59666112267405}\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"#Make one figure (CDF plot)\nimport matplotlib.pyplot as plt\n\ndef plot_latency_cdf(df_lat, title=\"Latency CDF (Trimmed Mean)\"):\n    g = np.sort(df_lat[df_lat[\"glasses\"]==1][\"lat_trim_ms\"].values)\n    n = np.sort(df_lat[df_lat[\"glasses\"]==0][\"lat_trim_ms\"].values)\n    yg = np.linspace(0, 1, len(g), endpoint=True)\n    yn = np.linspace(0, 1, len(n), endpoint=True)\n\n    plt.figure(figsize=(7,5))\n    plt.plot(g, yg, label=\"Eyeglasses\")\n    plt.plot(n, yn, label=\"No-Eyeglasses\")\n    plt.xlabel(\"Latency (ms)\")\n    plt.ylabel(\"CDF\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\nplot_latency_cdf(df_lat_final)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-12T02:30:08.439410Z","iopub.execute_input":"2026-01-12T02:30:08.440206Z","iopub.status.idle":"2026-01-12T02:30:08.591681Z","shell.execute_reply.started":"2026-01-12T02:30:08.440180Z","shell.execute_reply":"2026-01-12T02:30:08.591027Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 700x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}